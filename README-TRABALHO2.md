# ğŸ›¡ï¸ Trabalho 2: TolerÃ¢ncia a Falhas com ReplicaÃ§Ã£o

## ğŸ“– VisÃ£o Geral

Este trabalho implementa uma arquitetura distribuÃ­da com tolerÃ¢ncia a falhas para o serviÃ§o de transaÃ§Ãµes financeiras, usando replicaÃ§Ã£o de servidores e banco de dados.

## ğŸ¯ Objetivos

1. âœ… **Garantir alta disponibilidade** mesmo com falhas de componentes
2. âœ… **Implementar replicaÃ§Ã£o** de servidores web e banco de dados
3. âœ… **Avaliar impacto de falhas** atravÃ©s de experimentos controlados
4. âœ… **Documentar arquitetura** e decisÃµes de design

## ğŸ—ï¸ Arquitetura Implementada

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    NGINX Load Balancer              â”‚
â”‚                    (Round Robin)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                â”‚                â”‚
           â–¼                â–¼                â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  API 1   â”‚     â”‚  API 2   â”‚     â”‚  API 3   â”‚
    â”‚  :3001   â”‚     â”‚  :3002   â”‚     â”‚  :3003   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                â”‚                â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  PostgreSQL MASTER    â”‚
                â”‚        :5432          â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â”‚ Streaming Replication
                            â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  PostgreSQL REPLICA   â”‚
                â”‚        :5433          â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“Š Respostas Ã s QuestÃµes

### a) O que acontece se um dos servidores web ou do banco de dados falhar?

**Arquitetura Atual (Trabalho 1):**
- âŒ Ponto Ãºnico de falha
- âŒ Indisponibilidade total

**Arquitetura Proposta (Trabalho 2):**
- âœ… **Falha de 1 API:** Sistema continua funcionando (2 rÃ©plicas restantes)
- âœ… **Falha de 2 APIs:** Sistema degradado mas funcional (1 rÃ©plica)
- âš ï¸ **Falha do Master DB:** Downtime atÃ© restart (sem auto-failover)
- âœ… **Falha da Replica DB:** Sem impacto na disponibilidade

### b) Quantas rÃ©plicas? Como atualizar? Qual protocolo?

**RÃ©plicas:**
- **3 rÃ©plicas da API** (stateless)
- **1 Master + 1 Replica** do PostgreSQL

**Protocolo de ReplicaÃ§Ã£o:**
- **PostgreSQL Streaming Replication (AssÃ­ncrono)**
- WAL (Write-Ahead Log) transmitido continuamente
- Eventual consistency (~milissegundos de atraso)

**EstratÃ©gia de AtualizaÃ§Ã£o:**
- APIs: Load balancer distribui automaticamente (round-robin)
- Banco: Escritas â†’ Master | Leituras â†’ Master ou Replica

**Impacto:**
- âœ… Alta disponibilidade
- âœ… Escalabilidade horizontal
- âš ï¸ Complexidade aumentada
- âš ï¸ PossÃ­vel inconsistÃªncia temporÃ¡ria

### c) Servidores stateful ou stateless? Impacto?

**APIs:** **STATELESS**
- âœ… FÃ¡cil replicaÃ§Ã£o (rÃ©plicas idÃªnticas)
- âœ… Qualquer rÃ©plica pode atender qualquer requisiÃ§Ã£o
- âœ… Sem sticky sessions necessÃ¡rias
- âœ… Escalabilidade linear

**Banco de Dados:** **STATEFUL**
- âš ï¸ ReplicaÃ§Ã£o mais complexa
- âš ï¸ SincronizaÃ§Ã£o necessÃ¡ria
- âš ï¸ Failover requer promoÃ§Ã£o de replica
- âœ… SeparaÃ§Ã£o clara de responsabilidades

## ğŸš€ Quick Start

### 1. Instalar DependÃªncias

```bash
npm install
```

### 2. Iniciar Arquitetura Completa

```bash
docker-compose -f docker-compose-replication.yml up --build -d
```

### 3. Verificar Status

```bash
# Status dos containers
docker-compose -f docker-compose-replication.yml ps

# Health check
curl http://localhost/health
```

### 4. Popular Banco de Dados

```bash
docker exec -it financial-api-1 sh
npm run migration:run
npm run populate:20k
exit
```

### 5. Executar Testes de Falha

```bash
# Teste de falha de rÃ©plica da API
./scripts/run-teste-falha-api.sh

# Teste de falha do banco
./scripts/run-teste-falha-banco.sh

# Teste de mÃºltiplas falhas
./scripts/run-teste-multiplas-falhas.sh
```

## ğŸ§ª Experimentos

### Experimento 1: Baseline (Sem Falhas)
Estabelecer mÃ©tricas de referÃªncia.

```bash
docker run --rm -i --network host \
  grafana/k6 run - <k6/cenario-a-50-50.js
```

### Experimento 2: Falha de 1 RÃ©plica API

```bash
# Terminal 1: Iniciar teste
./scripts/run-teste-falha-api.sh

# Terminal 2: Aos 30s
docker stop financial-api-1
# Aos 50s
docker start financial-api-1
```

**Resultado Esperado:**
- âœ… 0% de downtime
- âš ï¸ ~10-20% aumento de latÃªncia
- âœ… RecuperaÃ§Ã£o automÃ¡tica

### Experimento 3: Falha de 2 RÃ©plicas API

```bash
# Simular falha de 2 rÃ©plicas
docker stop financial-api-1
docker stop financial-api-2
# Apenas api3 ativa - sistema degradado
```

**Resultado Esperado:**
- âœ… Sistema funciona com degradaÃ§Ã£o
- âš ï¸ ~50% aumento de latÃªncia
- âš ï¸ PossÃ­vel saturaÃ§Ã£o sob alta carga

### Experimento 4: Falha do Master DB

```bash
# Terminal 1: Iniciar teste
./scripts/run-teste-falha-banco.sh

# Terminal 2: Aos 30s
docker stop financial-db-master
# Aos 60s
docker start financial-db-master
```

**Resultado Esperado:**
- âŒ Downtime total (~30-45s)
- â±ï¸ DetecÃ§Ã£o: 5-10s
- â±ï¸ RecuperaÃ§Ã£o: 10-15s

### Experimento 5: Teste de Caos

```bash
./scripts/run-teste-multiplas-falhas.sh
# Seguir instruÃ§Ãµes no terminal
```

## ğŸ“Š MÃ©tricas Avaliadas

| MÃ©trica | DescriÃ§Ã£o |
|---------|-----------|
| **Disponibilidade** | % de uptime durante falhas |
| **LatÃªncia P95** | 95Âº percentil de tempo de resposta |
| **Throughput** | RequisiÃ§Ãµes/segundo |
| **Taxa de Erro** | % de requisiÃ§Ãµes falhadas |
| **Tempo de DetecÃ§Ã£o** | Tempo para detectar falha |
| **Tempo de RecuperaÃ§Ã£o** | Tempo para recuperar de falha |

## ğŸ“ Estrutura de Arquivos

```
â”œâ”€â”€ docker-compose-replication.yml  # OrquestraÃ§Ã£o completa
â”œâ”€â”€ nginx.conf                      # ConfiguraÃ§Ã£o load balancer
â”œâ”€â”€ database/
â”‚   â”œâ”€â”€ postgresql-master.conf     # Config master DB
â”‚   â”œâ”€â”€ postgresql-replica.conf    # Config replica DB
â”‚   â”œâ”€â”€ pg_hba.conf                # AutenticaÃ§Ã£o
â”‚   â”œâ”€â”€ init-master.sh             # Setup master
â”‚   â””â”€â”€ init-replica.sh            # Setup replica
â”œâ”€â”€ k6/
â”‚   â”œâ”€â”€ teste-falha-replica-api.js # Teste falha API
â”‚   â”œâ”€â”€ teste-falha-banco.js       # Teste falha DB
â”‚   â””â”€â”€ teste-multiplas-falhas.js  # Teste caos
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run-teste-falha-api.sh     # Executor teste API
â”‚   â”œâ”€â”€ run-teste-falha-banco.sh   # Executor teste DB
â”‚   â””â”€â”€ run-teste-multiplas-falhas.sh
â”œâ”€â”€ ARQUITETURA-TOLERANCIA-FALHAS.md
â”œâ”€â”€ GUIA-EXECUCAO-TRABALHO2.md
â””â”€â”€ TEMPLATE-RELATORIO-EXPERIMENTOS.md
```

## ğŸ“š DocumentaÃ§Ã£o

| Documento | DescriÃ§Ã£o |
|-----------|-----------|
| [ARQUITETURA-TOLERANCIA-FALHAS.md](./ARQUITETURA-TOLERANCIA-FALHAS.md) | AnÃ¡lise completa da arquitetura, decisÃµes de design e cenÃ¡rios de falha |
| [GUIA-EXECUCAO-TRABALHO2.md](./GUIA-EXECUCAO-TRABALHO2.md) | Guia passo-a-passo para executar e avaliar a soluÃ§Ã£o |
| [TEMPLATE-RELATORIO-EXPERIMENTOS.md](./TEMPLATE-RELATORIO-EXPERIMENTOS.md) | Template para documentar resultados dos experimentos |

## ğŸ”§ Comandos Ãšteis

### Monitoramento

```bash
# Logs em tempo real
docker-compose -f docker-compose-replication.yml logs -f

# Status dos containers
docker ps --format "table {{.Names}}\t{{.Status}}"

# Recursos utilizados
docker stats

# Verificar replicaÃ§Ã£o do PostgreSQL
docker exec -it financial-db-master psql -U finance_user -d financial_db -c "SELECT * FROM pg_stat_replication;"
```

### SimulaÃ§Ã£o de Falhas

```bash
# Derrubar rÃ©plica especÃ­fica
docker stop financial-api-1
docker stop financial-api-2
docker stop financial-api-3

# Derrubar banco
docker stop financial-db-master
docker stop financial-db-replica

# Recuperar serviÃ§os
docker start financial-api-1
docker start financial-db-master
```

### Limpeza

```bash
# Parar todos os serviÃ§os
docker-compose -f docker-compose-replication.yml down

# Remover volumes (CUIDADO: apaga dados)
docker-compose -f docker-compose-replication.yml down -v

# Rebuild completo
docker-compose -f docker-compose-replication.yml up --build --force-recreate
```

## âš ï¸ LimitaÃ§Ãµes Conhecidas

1. **Sem auto-failover do banco:** Falha do master requer intervenÃ§Ã£o manual
2. **ReplicaÃ§Ã£o assÃ­ncrona:** PossÃ­vel perda de transaÃ§Ãµes nÃ£o replicadas
3. **Sem monitoramento centralizado:** MÃ©tricas dependem do K6
4. **Sem circuit breaker:** Falhas em cascata possÃ­veis sob alta carga

## ğŸ”® Melhorias Futuras

- [ ] Implementar Patroni para auto-failover do PostgreSQL
- [ ] Adicionar Redis para cache distribuÃ­do
- [ ] Circuit breaker pattern nas APIs
- [ ] Prometheus + Grafana para monitoramento
- [ ] Distributed tracing (Jaeger)
- [ ] Backup automÃ¡tico e disaster recovery

## ğŸ“ ComparaÃ§Ã£o: Trabalho 1 vs Trabalho 2

| Aspecto | Trabalho 1 | Trabalho 2 |
|---------|------------|------------|
| **APIs** | 1 instÃ¢ncia | 3 instÃ¢ncias + LB |
| **Banco** | SQLite local | PostgreSQL com replica |
| **Disponibilidade** | ~90% (SPOF) | ~99%+ (tolerante) |
| **Escalabilidade** | Vertical | Horizontal |
| **Falha de API** | Downtime total | Sem impacto |
| **Falha de DB** | Downtime total | Downtime parcial |
| **Complexidade** | Baixa | Alta |
| **Custo** | Baixo | MÃ©dio |

## ğŸ“Š Resultados Esperados

### Disponibilidade por CenÃ¡rio

| CenÃ¡rio | Disponibilidade | Performance |
|---------|-----------------|-------------|
| Sistema Normal | 100% | 100% |
| 1 API down | 100% | 90-95% |
| 2 APIs down | 100% | 50-70% |
| Master DB down | 0% (downtime) | N/A |
| Replica DB down | 100% | 95-100% |

### Trade-offs Identificados

**ConsistÃªncia vs Disponibilidade:**
- Escolha: Disponibilidade (CAP Theorem)
- ReplicaÃ§Ã£o assÃ­ncrona favorece disponibilidade
- Eventual consistency aceitÃ¡vel (~ms)

**Simplicidade vs ResiliÃªncia:**
- Escolha: ResiliÃªncia
- Arquitetura mais complexa
- OperaÃ§Ã£o e manutenÃ§Ã£o mais sofisticadas

## ğŸ¤ ContribuiÃ§Ã£o

Para contribuir com melhorias:

1. Fork o repositÃ³rio
2. Crie uma branch: `git checkout -b melhoria-tolerancia-falhas`
3. Commit suas mudanÃ§as: `git commit -m 'Adiciona XYZ'`
4. Push para a branch: `git push origin melhoria-tolerancia-falhas`
5. Abra um Pull Request

## ğŸ“„ LicenÃ§a

Este projeto Ã© parte de um trabalho acadÃªmico.

## ğŸ‘¥ Autores

[PREENCHER COM INFORMAÃ‡Ã•ES DA EQUIPE]

## ğŸ“§ Contato

[PREENCHER]

---

**Nota:** Este Ã© o Trabalho 2 que estende o Trabalho 1 com capacidades de tolerÃ¢ncia a falhas. Para executar o sistema original (sem replicaÃ§Ã£o), use `docker-compose.yml` em vez de `docker-compose-replication.yml`.
